{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick notes on the concept which were not in Deep Learning modules of Machine Learning Nano Degree:\n",
    "\n",
    "##### Regarding DL for Text :\n",
    "* In text dataset, key terms are more important and we might no have enough dataset describing them. Deep Learning model usually expects many samples to understand the variance of a label. like image of a perticular flower.\n",
    "* Semantic relation between kitty and cat understanding also requires huge dataset for DL. If 2 words are similar then we have to share weights between them. i.e. the model should predict they are same when asked so the the internal weights of the model should indicate that the weights values are such that out of so huge NN (CNN) the trajectory of the word transformation for both the words are same.\n",
    "\n",
    "##### similar words occur in similar context:\n",
    "* As we have lot of text from the the wikipedia, we can have unsupervised learning to identify the similar words with **the assumption that similar words occur in similar context.** \n",
    "* This assumption helps in 2 ways :\n",
    "    * without knowing their meaning we can have associated words.\n",
    "    * representation of words can be done through the associated words rather than sparse matrix of n words with n words. This representation of words by associated word vector is called **embedding**. Clearly, this is smaller vector from the sparse matrix.\n",
    "    * with 1 word embedding we have multiple words association which are in the word embedding is obtained.\n",
    "    \n",
    "##### How do we get the word embedding?\n",
    "* **Word2Vec**: split a sentence it to 2 sets or words. One set of words trying to predict other set of words. Here simple logistic regression can be used to generate the model for prediction. The best set of word (also called window of words) that best represents the corpus (sentences, paragraph or book) is then saved.\n",
    "    * **skip grams**: predicts the neighboring context from a word\n",
    "    * **Continuous Bag Of Words**: predicts the word from neighboring context.\n",
    "    * https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
    "    * **Sampled Softmax**: When vocabulary of the target words are large, softmax may be in-efficient. Therefore, **non class targets** are randomly sampled and with this small subset used for training the model.\n",
    "\n",
    "* **t-SNE**: is a visualization technique where the association of words are projected onto 2D with relation between the words are retained. Closer words are plotted closer and loosely coupled words are plotted far away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### So, now, we have word embedding...what next ?\n",
    "* compare them using cosine similarity. Why compare? thats the most common activity on text. Applications like plagarism check, classifying documents based on authors require word embedding to be compared.\n",
    "\n",
    "\n",
    "* can it also help in Q&A ? predicting next sequence of sentences ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RNN: Recurring Neural Network\n",
    "* takes into consideration of input vector time. i.e. what was the state of input at t-1th instance. it shares weights across time rather than space as in CNN.\n",
    "* since input changes over time and at any point it depends on previous state, a model is used to summarized the events at past. This model forms the recurrent connetion.\n",
    "* A repeatable network of recurrent connection to summarize the past, input vector and predicting model (classification or regression) is called **RNN**. Usually the predicting model is same in the repeated network.\n",
    "* Here input vector remains the same but the tth second we would take into consider recursive transformation over t-1 and before.\n",
    "* **back propagation over time** would result in correlated update to weights which would result in bad gradient descent. Correlated update is because of recurrent network which has the wieght carried in the output of t-1 second.\n",
    "\n",
    "* **vanishing gradients**: here gradients abruptly decreases to zero. This would result no training to model. Also result in memory loss and model do not keep tracks of more distant past and keeps track of only recent past.\n",
    "\n",
    "* **exploding gradients**: here gradients abruptly increases to infinity. This can be addressed by Gradient clipping where gradients are normalized and when gradients are too huge the steps are cut shorted. i.e. when u start back propagating the point at which it reaches the max threshold u just stop increasing the gradients and the same value is assumed for t-x and less.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM: Long Short Term Memory\n",
    "* addresses the vanishing gradients problem in RNN\n",
    "* replaces the predicting model in RNN with LSTM \"cell\" which does the memory management. This memory management helps network to remember the older events. Thus solving vanishing gradient problem.\n",
    "* memory management operation involves storing, forgetting and reading memory.\n",
    "* instead of making those operation descrete result oriented, it is controlled to through logistic regression which makes it easier for partial read, store and delete.\n",
    "* **regularization**: L2 regularization can be used on all 4 sides (2 inputs and 2 outputs). Dropout can be used in only input and/or output and not between recurring network and connect future areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RNN Application : Beam Search\n",
    "* RNN are for time series activities prediction, one example is word prediction like that one Google search bar does.\n",
    "* It takes the input of so far entered text and predicts the upcoming word or characters.\n",
    "* Taking one prediction at a time would be too greedy, rather to take couple of them and continue the sequence for all of them.\n",
    "* this would mean that there are now multiple sequence being investigated \n",
    "* Now based on the final probabilities score, one sequence can be finalized\n",
    "* This approach is good to avoid scenario which accidentally landing to one word, but now we have couple sequences to choose from.\n",
    "\n",
    "e.g.: App can lead to both apple or append or application. Now just because one model chooses,say apple, because its probability is slightly larger than other you would loose option of other 2 words.\n",
    "\n",
    "If you had chosen both 'l' & 'e' and continued series then you would have at the end to choose from apple, append and application. Now based on the probability score (multiply all the predicted probabilities) you are in better position to choose the word.\n",
    "\n",
    "* Maintaining all sequences might not be cost effective(time and memory). For this we have Beam search.\n",
    "* it prunes the sequence which are not so likely and keeps only the most likely. It define a beam width which defines the most likely sequence window there by simulating beam like projection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM through Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46, 48], [40, 9], [32, 7], [40, 9], [4], [25], [39, 7], [22, 40], [39, 9], [41, 6, 48, 38]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46 48  0  0]\n",
      " [40  9  0  0]\n",
      " [32  7  0  0]\n",
      " [40  9  0  0]\n",
      " [ 4  0  0  0]\n",
      " [25  0  0  0]\n",
      " [39  7  0  0]\n",
      " [22 40  0  0]\n",
      " [39  9  0  0]\n",
      " [41  6 48 38]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\padmaraj.bhat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Embedding\n",
    "#from tensorflow.keras.embeddings import Embedding\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 100us/sample - loss: 0.2993 - acc: 1.0000\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82694954],\n",
       "       [0.77747256],\n",
       "       [0.7040919 ],\n",
       "       [0.77747256],\n",
       "       [0.6916161 ],\n",
       "       [0.28503722],\n",
       "       [0.29529685],\n",
       "       [0.23452783],\n",
       "       [0.42103335],\n",
       "       [0.07797129]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just with \"not\" what is the score ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27370414]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([22,0,0,0]).reshape(1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### it is near to score and hence negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just with \"great\", what is the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6872823]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([32,0,0,0]).reshape(1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment prediction is \"positive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just with \"work\", what is the score ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6872823]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([32,0,0,0]).reshape(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5001465]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([0,32,0,0]).reshape(1,4)) # work as 2nd word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30238178]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([22,32,0,0]).reshape(1,4)) # not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30804333]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([39,32,0,0]).reshape(1,4)) #poor work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* with \"work\" as first word it is predicting positive sentiment\n",
    "* with \"work\" as the second word it is predicting negative sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if there are words as labels?\n",
    "* Like quotes fo the sentence and author of the sentence as labels ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_str = [(\"A for\", \"Apple\")\n",
    "            , (\"B for\", \"Ball\")\n",
    "            , (\"C for\", \"cat\")\n",
    "            , (\"Kitty can also mean\", \"cat\")\n",
    "            , (\" meow sound can also mean\", \"cat\")\n",
    "            , (\"D for\", \"Dog\")\n",
    "            , (\"A is also for\", \"Ant\")\n",
    "            , (\"D can also have\", \"Drum\")\n",
    "            , (\"B is popularly known shortcut for\", \"Be\")\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* here a simple classifier can map the the sequence of text based on length or BoW can lead to a class. I guess this not a good example for LSTM.\n",
    "\n",
    "* Here if the model is formed it might predict for \"D is also know for\" as \"Ant\". Let us check !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24, 18], [19, 18], [90, 18], [27, 40, 28, 29], [31, 95, 40, 28, 29], [77, 18], [24, 43, 28, 18], [77, 40, 28, 46], [19, 43, 6, 14, 3, 18]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 100\n",
    "encoded_docs = [one_hot(d[0], vocab_size) for d in docs_str]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24 18  0  0  0  0]\n",
      " [19 18  0  0  0  0]\n",
      " [90 18  0  0  0  0]\n",
      " [27 40 28 29  0  0]\n",
      " [31 95 40 28 29  0]\n",
      " [77 18  0  0  0  0]\n",
      " [24 43 28 18  0  0]\n",
      " [77 40 28 46  0  0]\n",
      " [19 43  6 14  3 18]]\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(x) for x in encoded_docs])\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 6, 20)             2000      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 2,121\n",
      "Trainable params: 2,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 20, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 53, 78, 78, 78, 49, 85, 70, 23]\n",
      "9/9 [==============================] - 0s 111us/sample - loss: -708.7500 - acc: 0.0000e+00\n",
      "Accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "labels = [ one_hot(y, vocab_size)[0] for y in [x[1] for x in docs_str] ]\n",
    "print(labels)\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 6 6 6 4 0 5 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "labels = [x[1] for x in docs_str]\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padmaraj.bhat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "onehot_encoded = onehot_encoder.fit_transform(np.array(labels).reshape(-1,1))\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 6, 30)             3000      \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 1267      \n",
      "=================================================================\n",
      "Total params: 4,267\n",
      "Trainable params: 4,267\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 30, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(7, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 111us/sample - loss: 0.2301 - acc: 1.0000\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, onehot_encoded, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, onehot_encoded, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01943946, 0.35496178, 0.04048574, 0.0042848 , 0.04294088,\n",
       "        0.00186032, 0.02373973],\n",
       "       [0.00214723, 0.03345937, 0.31550813, 0.0127781 , 0.01372948,\n",
       "        0.00122043, 0.06902653],\n",
       "       [0.00129768, 0.01842001, 0.05497   , 0.00307879, 0.04996714,\n",
       "        0.00225902, 0.33951503],\n",
       "       [0.00857687, 0.00259757, 0.00412455, 0.00419933, 0.00232127,\n",
       "        0.02179644, 0.5118555 ],\n",
       "       [0.00760907, 0.00786635, 0.00805849, 0.01714033, 0.00963524,\n",
       "        0.01276016, 0.90130043],\n",
       "       [0.00285327, 0.0481348 , 0.02228722, 0.00250304, 0.36661592,\n",
       "        0.01408225, 0.06488186],\n",
       "       [0.5527029 , 0.01783398, 0.00162145, 0.01724142, 0.00420105,\n",
       "        0.01095372, 0.01680672],\n",
       "       [0.02444848, 0.00358361, 0.00100103, 0.00916204, 0.01780856,\n",
       "        0.6778833 , 0.07911226],\n",
       "       [0.07609388, 0.01595449, 0.0753158 , 0.9118255 , 0.00963558,\n",
       "        0.03528861, 0.08005422]], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text : [77, 43, 28, 14, 18]\n",
      "[[77 43 28 14 18  0]]\n"
     ]
    }
   ],
   "source": [
    "text = \"D is also known for\"\n",
    "encoded_text = one_hot(text, vocab_size)\n",
    "print(\"Encoded text :\", encoded_text)\n",
    "padded_text = pad_sequences([encoded_text], maxlen=max_length, padding='post')\n",
    "print(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27758425, 0.00707912, 0.00233823, 0.08292675, 0.14962743,\n",
       "        0.32138762, 0.07658859]], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text : [77, 43, 14, 3, 18]\n",
      "[[77 43 14  3 18  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.18166593, 0.04393473, 0.01495257, 0.05598897, 0.3390646 ,\n",
       "        0.28406417, 0.21930079]], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"D is known shortcut for\"\n",
    "encoded_text = one_hot(text, vocab_size)\n",
    "print(\"Encoded text :\", encoded_text)\n",
    "padded_text = pad_sequences([encoded_text], maxlen=max_length, padding='post')\n",
    "print(padded_text)\n",
    "model.predict(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text : [9, 43, 14, 3, 18]\n",
      "[[ 9 43 14  3 18  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.22576067, 0.06020024, 0.04715523, 0.11328119, 0.05352072,\n",
       "        0.07511067, 0.26769105]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"F is known shortcut for\"\n",
    "encoded_text = one_hot(text, vocab_size)\n",
    "print(\"Encoded text :\", encoded_text)\n",
    "padded_text = pad_sequences([encoded_text], maxlen=max_length, padding='post')\n",
    "print(padded_text)\n",
    "model.predict(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text : [76, 48, 82, 3, 15, 43, 18]\n",
      "[[48 82  3 15 43 18]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.22029433, 0.3572051 , 0.27860332, 0.38095355, 0.30702597,\n",
       "        0.20268635, 0.33346674]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I dont know which one is this\"\n",
    "encoded_text = one_hot(text, vocab_size)\n",
    "print(\"Encoded text :\", encoded_text)\n",
    "padded_text = pad_sequences([encoded_text], maxlen=max_length, padding='post')\n",
    "print(padded_text)\n",
    "model.predict(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text : [86]\n",
      "[[86  0  0  0  0  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00523269, 0.03013298, 0.03118229, 0.00637817, 0.02094114,\n",
       "        0.00545558, 0.14406063]], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"what?\"\n",
    "encoded_text = one_hot(text, vocab_size)\n",
    "print(\"Encoded text :\", encoded_text)\n",
    "padded_text = pad_sequences([encoded_text], maxlen=max_length, padding='post')\n",
    "print(padded_text)\n",
    "model.predict(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text : [86, 43, 18]\n",
      "[[86 43 18  0  0  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03901008, 0.01958153, 0.02946073, 0.03354695, 0.03508037,\n",
       "        0.0135765 , 0.09712441]], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"what is this?\"\n",
    "encoded_text = one_hot(text, vocab_size)\n",
    "print(\"Encoded text :\", encoded_text)\n",
    "padded_text = pad_sequences([encoded_text], maxlen=max_length, padding='post')\n",
    "print(padded_text)\n",
    "model.predict(padded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Why  would we choose LSTM in this case? We could do it without that too right ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After an hours of search .... I could not get a precise example where it is absolutely necessary to use LSTM (or RNN).\n",
    "\n",
    "##### It is always claimed that it gave good results in text translation and music notes prediction and in games but I see it as below\n",
    "* why would not a simple which does the classification be used iteravatively to produce the same result as that of LSTM.\n",
    "    * e.g.: I can iteratively get the probability of the subsequent of characters from a trained model.\n",
    "* if memory is the advantage in LSTM, then why not a simple search give better result or at least same result as that of LSTM.\n",
    "    * e.g.: I can SEARCH the series of musical note and predict the next note. Here, I agree that the search time is much more than the prediction time of the LSTM but I guess we can always improvise search techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: Not really an answer. The outro to the lesson carefully make the same point above. That when we have enough data and computation power, we can mix various technique or more professionally indicated as \"layers or techniques\" or \"stacks of techniques\" to build a new model which *OFTEN* does better job that hand crafted approaches to the problem.\n",
    "\n",
    "* This indirectly tells me that we can have simplified approach and machine learning approach but machine learning can be fine tuned with various techniques like number of layers, number neurons, activation function etc. However, in naive approach it has to be industry or data scientist solving the problem with careful coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Models as Lego:\n",
    "* Deep neural network is nothing but hidden nueral network layers; now it is upto you how do you want to form layers. it can be RNN, LSTM and CNN combinations. \n",
    "* Why ?: e.g.: for a speech recognition you can optimize the speech recognition pattern  to complete the translation. so you may have cnn for the speech recognition but later lstm to understand the missing sequence pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick note on CNN and maxpool:\n",
    "- CNN is a filter to recognize the elements.\n",
    "- Max pool is to downsize the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Outlier:\n",
    "* we know that plotting is the best way to know the outlier but a isoloated point does not indicate it is an outlier\n",
    "    * e.g.: in fraud detection you are expected to have the fraud characterstic outside the normal\n",
    "    * in rent prediction higher sqft usually have has hight rent or saleprice\n",
    "        * even if the sqft is high based on the neighborhood house rent may be low for that area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick notes on NLTK, Gensim TF-IDF generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem = WordNetLemmatizer()\n",
    "print(stem.lemmatize(\"working\"))\n",
    "\n",
    "stem = SnowballStemmer('english')\n",
    "stem.stem('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'abstract', 'all', 'also', 'amounts', 'analysis', 'and', 'are', 'around', 'as', 'attempts', 'be', 'because', 'been', 'behind', 'between', 'but', 'by', 'can', 'cannot', 'capture', 'clusters', 'comes', 'compelling', 'concept', 'concepts', 'context', 'data', 'decompose', 'decomposition', 'different', 'difficult', 'discovering', 'distinguish', 'document', 'documents', 'each', 'easily', 'engines', 'enough', 'even', 'every', 'exploring', 'finding', 'for', 'getting', 'have', 'helps', 'hidden', 'however', 'humans', 'if', 'in', 'include', 'interesting', 'into', 'intricacies', 'is', 'isn', 'it', 'known', 'languages', 'large', 'latent', 'leverage', 'lsa', 'machine', 'matched', 'matrices', 'matrix', 'mean', 'meanings', 'misunderstood', 'modeling', 'multiple', 'next', 'not', 'nuances', 'of', 'on', 'own', 'perform', 'play', 'quite', 're', 'read', 'reasons', 'represent', 'results', 'same', 'search', 'semantic', 'similarity', 'sklearn', 'sometimes', 'spelling', 'step', 'string', 'task', 'term', 'text', 'that', 'the', 'their', 'then', 'these', 'they', 'thing', 'this', 'to', 'topic', 'topics', 'truncatedsvd', 'understand', 'us', 'use', 'used', 'vector', 'we', 'well', 'weren', 'where', 'wherein', 'which', 'will', 'with', 'words', 'would']\n",
      "(4, 128)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'Topic modeling helps in exploring large amounts of text data, finding clusters of words, the similarity between documents, and discovering abstract topics. As if these reasons weren’t compelling enough, topic modeling is also used in search engines wherein the search string is matched with the results. Getting interesting, isn’t it? Well, read on then!',\n",
    "    'All languages have their own intricacies and nuances which are quite difficult for a machine to capture (sometimes they’re even misunderstood by us humans!). This can include different words that mean the same thing, and also the words which have the same spelling but different meanings.',\n",
    "    'We can easily distinguish between these words because we are able to understand the context behind these words. However, a machine would not be able to capture this concept as it cannot understand the context in which the words have been used. This is where Latent Semantic Analysis (LSA) comes into play as it attempts to leverage the context around the words to capture the hidden concepts, also known as topics.',\n",
    "    'The next step is to represent each and every term and document as a vector. We will use the document-term matrix and decompose it into multiple matrices. We will use sklearn’s TruncatedSVD to perform the task of matrix decomposition.',\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.13413859, 0.        , 0.08561892, 0.13413859,\n",
       "         0.        , 0.08561892, 0.        , 0.        , 0.08561892,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.1057564 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.13413859, 0.        , 0.13413859, 0.        ,\n",
       "         0.        , 0.        , 0.13413859, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.13413859, 0.        , 0.        ,\n",
       "         0.13413859, 0.        , 0.        , 0.13413859, 0.13413859,\n",
       "         0.        , 0.        , 0.13413859, 0.13413859, 0.        ,\n",
       "         0.13413859, 0.        , 0.13413859, 0.        , 0.        ,\n",
       "         0.        , 0.13413859, 0.21151281, 0.        , 0.13413859,\n",
       "         0.        , 0.        , 0.17123785, 0.13413859, 0.08561892,\n",
       "         0.        , 0.        , 0.13413859, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.13413859, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.26827718, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.21151281, 0.13413859,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.13413859, 0.13413859, 0.        , 0.13413859, 0.        ,\n",
       "         0.26827718, 0.        , 0.13413859, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.13413859, 0.        , 0.        ,\n",
       "         0.13413859, 0.        , 0.20999724, 0.        , 0.13413859,\n",
       "         0.1057564 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.26827718, 0.1057564 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.1057564 , 0.        , 0.        , 0.13413859,\n",
       "         0.13413859, 0.        , 0.13413859, 0.        , 0.        ,\n",
       "         0.13413859, 0.08561892, 0.        ],\n",
       "        [0.        , 0.        , 0.14648076, 0.09349677, 0.        ,\n",
       "         0.        , 0.18699355, 0.11548712, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.14648076, 0.14648076, 0.11548712, 0.        ,\n",
       "         0.11548712, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.29296153, 0.14648076, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.14648076, 0.        , 0.        , 0.        , 0.14648076,\n",
       "         0.        , 0.23097423, 0.        , 0.        , 0.        ,\n",
       "         0.14648076, 0.        , 0.        , 0.14648076, 0.        ,\n",
       "         0.        , 0.14648076, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.14648076, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.11548712, 0.        , 0.        , 0.        ,\n",
       "         0.14648076, 0.14648076, 0.14648076, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.14648076, 0.        , 0.        ,\n",
       "         0.14648076, 0.        , 0.        , 0.14648076, 0.14648076,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.29296153,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.14648076,\n",
       "         0.14648076, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.14648076, 0.22931922, 0.14648076, 0.        ,\n",
       "         0.        , 0.14648076, 0.14648076, 0.11548712, 0.09349677,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.14648076,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.23097423, 0.        ,\n",
       "         0.        , 0.18699355, 0.        ],\n",
       "        [0.21552379, 0.        , 0.        , 0.06878302, 0.        ,\n",
       "         0.1077619 , 0.        , 0.08496072, 0.1077619 , 0.20634907,\n",
       "         0.1077619 , 0.1077619 , 0.1077619 , 0.1077619 , 0.1077619 ,\n",
       "         0.08496072, 0.        , 0.        , 0.08496072, 0.1077619 ,\n",
       "         0.16992143, 0.        , 0.1077619 , 0.        , 0.1077619 ,\n",
       "         0.1077619 , 0.32328569, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.1077619 , 0.        ,\n",
       "         0.        , 0.        , 0.1077619 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.08496072, 0.        , 0.1077619 , 0.1077619 ,\n",
       "         0.        , 0.        , 0.08496072, 0.        , 0.        ,\n",
       "         0.08496072, 0.        , 0.06878302, 0.        , 0.13756605,\n",
       "         0.1077619 , 0.        , 0.        , 0.1077619 , 0.1077619 ,\n",
       "         0.1077619 , 0.08496072, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.1077619 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.1077619 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.1077619 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.33740777, 0.        , 0.        ,\n",
       "         0.16992143, 0.        , 0.        , 0.16992143, 0.27513209,\n",
       "         0.        , 0.08496072, 0.        , 0.21552379, 0.        ,\n",
       "         0.        , 0.08496072, 0.        , 0.16992143, 0.        ,\n",
       "         0.        , 0.1077619 , 0.        , 0.08496072, 0.        ,\n",
       "         0.        , 0.27513209, 0.1077619 ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.28020995, 0.        , 0.        , 0.09340332,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.14633434, 0.14633434,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.29266869,\n",
       "         0.        , 0.14633434, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.14633434, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.11537168, 0.        , 0.09340332, 0.        , 0.09340332,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.14633434, 0.29266869,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.14633434,\n",
       "         0.14633434, 0.        , 0.        , 0.11537168, 0.        ,\n",
       "         0.        , 0.14633434, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.14633434, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.14633434, 0.        ,\n",
       "         0.        , 0.14633434, 0.        , 0.14633434, 0.29266869,\n",
       "         0.        , 0.        , 0.22908999, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.18680663,\n",
       "         0.        , 0.        , 0.14633434, 0.        , 0.        ,\n",
       "         0.29266869, 0.        , 0.14633434, 0.23074335, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.29266869,\n",
       "         0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 128)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'able capture', 'able capture concept', 'able capture concept understand', 'able understand', 'able understand context', 'able understand context words', 'abstract', 'abstract topics', 'abstract topics reasons', 'abstract topics reasons weren', 'amounts', 'amounts text', 'amounts text data', 'amounts text data finding', 'analysis', 'analysis lsa', 'analysis lsa comes', 'analysis lsa comes play', 'attempts', 'attempts leverage', 'attempts leverage context', 'attempts leverage context words', 'capture', 'capture concept', 'capture concept understand', 'capture concept understand context', 'capture hidden', 'capture hidden concepts', 'capture hidden concepts known', 'capture misunderstood', 'capture misunderstood humans', 'capture misunderstood humans include', 'clusters', 'clusters words', 'clusters words similarity', 'clusters words similarity documents', 'comes', 'comes play', 'comes play attempts', 'comes play attempts leverage', 'compelling', 'compelling topic', 'compelling topic modeling', 'compelling topic modeling used', 'concept', 'concept understand', 'concept understand context', 'concept understand context words', 'concepts', 'concepts known', 'concepts known topics', 'context', 'context words', 'context words capture', 'context words capture hidden', 'context words machine', 'context words machine able', 'context words used', 'context words used latent', 'data', 'data finding', 'data finding clusters', 'data finding clusters words', 'decompose', 'decompose multiple', 'decompose multiple matrices', 'decompose multiple matrices use', 'decomposition', 'different', 'different meanings', 'different words', 'different words mean', 'different words mean thing', 'difficult', 'difficult machine', 'difficult machine capture', 'difficult machine capture misunderstood', 'discovering', 'discovering abstract', 'discovering abstract topics', 'discovering abstract topics reasons', 'distinguish', 'distinguish words', 'distinguish words able', 'distinguish words able understand', 'document', 'document term', 'document term matrix', 'document term matrix decompose', 'document vector', 'document vector use', 'document vector use document', 'documents', 'documents discovering', 'documents discovering abstract', 'documents discovering abstract topics', 'easily', 'easily distinguish', 'easily distinguish words', 'easily distinguish words able', 'engines', 'engines search', 'engines search string', 'engines search string matched', 'exploring', 'exploring large', 'exploring large amounts', 'exploring large amounts text', 'finding', 'finding clusters', 'finding clusters words', 'finding clusters words similarity', 'getting', 'getting interesting', 'getting interesting isn', 'getting interesting isn read', 'helps', 'helps exploring', 'helps exploring large', 'helps exploring large amounts', 'hidden', 'hidden concepts', 'hidden concepts known', 'hidden concepts known topics', 'humans', 'humans include', 'humans include different', 'humans include different words', 'include', 'include different', 'include different words', 'include different words mean', 'interesting', 'interesting isn', 'interesting isn read', 'intricacies', 'intricacies nuances', 'intricacies nuances quite', 'intricacies nuances quite difficult', 'isn', 'isn read', 'known', 'known topics', 'languages', 'languages intricacies', 'languages intricacies nuances', 'languages intricacies nuances quite', 'large', 'large amounts', 'large amounts text', 'large amounts text data', 'latent', 'latent semantic', 'latent semantic analysis', 'latent semantic analysis lsa', 'leverage', 'leverage context', 'leverage context words', 'leverage context words capture', 'lsa', 'lsa comes', 'lsa comes play', 'lsa comes play attempts', 'machine', 'machine able', 'machine able capture', 'machine able capture concept', 'machine capture', 'machine capture misunderstood', 'machine capture misunderstood humans', 'matched', 'matched results', 'matched results getting', 'matched results getting interesting', 'matrices', 'matrices use', 'matrices use sklearn', 'matrices use sklearn truncatedsvd', 'matrix', 'matrix decompose', 'matrix decompose multiple', 'matrix decompose multiple matrices', 'matrix decomposition', 'mean', 'mean thing', 'mean thing words', 'mean thing words spelling', 'meanings', 'misunderstood', 'misunderstood humans', 'misunderstood humans include', 'misunderstood humans include different', 'modeling', 'modeling helps', 'modeling helps exploring', 'modeling helps exploring large', 'modeling used', 'modeling used search', 'modeling used search engines', 'multiple', 'multiple matrices', 'multiple matrices use', 'multiple matrices use sklearn', 'nuances', 'nuances quite', 'nuances quite difficult', 'nuances quite difficult machine', 'perform', 'perform task', 'perform task matrix', 'perform task matrix decomposition', 'play', 'play attempts', 'play attempts leverage', 'play attempts leverage context', 'quite', 'quite difficult', 'quite difficult machine', 'quite difficult machine capture', 'read', 'reasons', 'reasons weren', 'reasons weren compelling', 'reasons weren compelling topic', 'represent', 'represent term', 'represent term document', 'represent term document vector', 'results', 'results getting', 'results getting interesting', 'results getting interesting isn', 'search', 'search engines', 'search engines search', 'search engines search string', 'search string', 'search string matched', 'search string matched results', 'semantic', 'semantic analysis', 'semantic analysis lsa', 'semantic analysis lsa comes', 'similarity', 'similarity documents', 'similarity documents discovering', 'similarity documents discovering abstract', 'sklearn', 'sklearn truncatedsvd', 'sklearn truncatedsvd perform', 'sklearn truncatedsvd perform task', 'spelling', 'spelling different', 'spelling different meanings', 'step', 'step represent', 'step represent term', 'step represent term document', 'string', 'string matched', 'string matched results', 'string matched results getting', 'task', 'task matrix', 'task matrix decomposition', 'term', 'term document', 'term document vector', 'term document vector use', 'term matrix', 'term matrix decompose', 'term matrix decompose multiple', 'text', 'text data', 'text data finding', 'text data finding clusters', 'thing', 'thing words', 'thing words spelling', 'thing words spelling different', 'topic', 'topic modeling', 'topic modeling helps', 'topic modeling helps exploring', 'topic modeling used', 'topic modeling used search', 'topics', 'topics reasons', 'topics reasons weren', 'topics reasons weren compelling', 'truncatedsvd', 'truncatedsvd perform', 'truncatedsvd perform task', 'truncatedsvd perform task matrix', 'understand', 'understand context', 'understand context words', 'understand context words machine', 'understand context words used', 'use', 'use document', 'use document term', 'use document term matrix', 'use sklearn', 'use sklearn truncatedsvd', 'use sklearn truncatedsvd perform', 'used', 'used latent', 'used latent semantic', 'used latent semantic analysis', 'used search', 'used search engines', 'used search engines search', 'vector', 'vector use', 'vector use document', 'vector use document term', 'weren', 'weren compelling', 'weren compelling topic', 'weren compelling topic modeling', 'words', 'words able', 'words able understand', 'words able understand context', 'words capture', 'words capture hidden', 'words capture hidden concepts', 'words machine', 'words machine able', 'words machine able capture', 'words mean', 'words mean thing', 'words mean thing words', 'words similarity', 'words similarity documents', 'words similarity documents discovering', 'words spelling', 'words spelling different', 'words spelling different meanings', 'words used', 'words used latent', 'words used latent semantic']\n",
      "(4, 344)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,4))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['topic', 'modeling', 'helps', 'exploring', 'large', 'amounts', 'text', 'data', 'finding', 'clusters', 'words', 'similarity', 'documents', 'discovering', 'abstract', 'topics', 'as', 'reasons', 'weren', 'compelling', 'enough', 'topic', 'modeling', 'search', 'engines', 'search', 'string', 'matched', 'results', 'getting', 'interesting', 'isn', 'it', 'well', 'read', 'then']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "corpus_gen = [gensim.utils.simple_preprocess(remove_stopwords(doc))  for doc in corpus]\n",
    "print(corpus_gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = Dictionary(corpus_gen)\n",
    "corpus_doc2bow = [dct.doc2bow(line) for line in corpus_gen]  # convert corpus to BoW format\n",
    "model = TfidfModel(corpus_doc2bow)  # fit model\n",
    "vector = model[corpus_doc2bow]  # apply model to the first corpus document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([dct.get(i) for doc in vector for i, value in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well',\n",
       " 'weren',\n",
       " 'modeling',\n",
       " 'search',\n",
       " 'topic',\n",
       " 'sometimes',\n",
       " 'spelling',\n",
       " 'they',\n",
       " 'thing',\n",
       " 'different',\n",
       " 'semantic',\n",
       " 'used',\n",
       " 'able',\n",
       " 'understand',\n",
       " 'context',\n",
       " 'vector',\n",
       " 'document',\n",
       " 'matrix',\n",
       " 'term',\n",
       " 'use']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dct.get(i) for doc in vector for i, value in sorted(doc, key=lambda x:x[1])[-5:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_gen = gensim.parsing.preprocessing.preprocess_documents(corpus)\n",
    "dct = Dictionary(corpus_gen)\n",
    "corpus_doc2bow = [dct.doc2bow(line) for line in corpus_gen]  # convert corpus to BoW format\n",
    "model = TfidfModel(corpus_doc2bow)  # fit model\n",
    "vector = model[corpus_doc2bow]  # apply model to the first corpus document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([dct.get(i) for doc in vector for i, value in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract',\n",
       " 'amount',\n",
       " 'cluster',\n",
       " 'compel',\n",
       " 'data',\n",
       " 'discov',\n",
       " 'document',\n",
       " 'engin',\n",
       " 'explor',\n",
       " 'find',\n",
       " 'get',\n",
       " 'help',\n",
       " 'interest',\n",
       " 'isn’t',\n",
       " 'larg',\n",
       " 'match',\n",
       " 'model',\n",
       " 'read',\n",
       " 'reason',\n",
       " 'result',\n",
       " 'search',\n",
       " 'similar',\n",
       " 'string',\n",
       " 'text',\n",
       " 'topic',\n",
       " 'weren’t',\n",
       " 'word',\n",
       " 'word',\n",
       " 'captur',\n",
       " 'differ',\n",
       " 'difficult',\n",
       " 'human',\n",
       " 'includ',\n",
       " 'intricaci',\n",
       " 'languag',\n",
       " 'machin',\n",
       " 'mean',\n",
       " 'misunderstood',\n",
       " 'nuanc',\n",
       " 'spell',\n",
       " 'they’r',\n",
       " 'thing',\n",
       " 'topic',\n",
       " 'word',\n",
       " 'captur',\n",
       " 'machin',\n",
       " 'abl',\n",
       " 'analysi',\n",
       " 'attempt',\n",
       " 'come',\n",
       " 'concept',\n",
       " 'context',\n",
       " 'distinguish',\n",
       " 'easili',\n",
       " 'hidden',\n",
       " 'known',\n",
       " 'latent',\n",
       " 'leverag',\n",
       " 'lsa',\n",
       " 'plai',\n",
       " 'semant',\n",
       " 'understand',\n",
       " 'document',\n",
       " 'decompos',\n",
       " 'decomposit',\n",
       " 'matric',\n",
       " 'matrix',\n",
       " 'multipl',\n",
       " 'perform',\n",
       " 'repres',\n",
       " 'sklearn’',\n",
       " 'step',\n",
       " 'task',\n",
       " 'term',\n",
       " 'truncatedsvd',\n",
       " 'us',\n",
       " 'vector']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dct.get(i) for doc in vector for i, value in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('includ', 0.2314203798197213),\n",
       " ('intricaci', 0.2314203798197213),\n",
       " ('languag', 0.2314203798197213),\n",
       " ('misunderstood', 0.2314203798197213),\n",
       " ('nuanc', 0.2314203798197213),\n",
       " ('spell', 0.2314203798197213),\n",
       " ('they’r', 0.2314203798197213),\n",
       " ('thing', 0.2314203798197213),\n",
       " ('topic', 0.26294325735857826),\n",
       " ('abl', 0.33715249805902553),\n",
       " ('concept', 0.33715249805902553),\n",
       " ('understand', 0.33715249805902553),\n",
       " ('model', 0.3505910098114377),\n",
       " ('search', 0.3505910098114377),\n",
       " ('matrix', 0.4082482904638631),\n",
       " ('term', 0.4082482904638631),\n",
       " ('us', 0.4082482904638631),\n",
       " ('differ', 0.4628407596394426),\n",
       " ('mean', 0.4628407596394426),\n",
       " ('context', 0.5057287470885382)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_among_docs = [(i,value) for doc in vector for i, value in sorted(doc, key=lambda x:x[1])[-10:]]\n",
    "sorted([(dct.get(i),value) for i,value in sorted(top_among_docs, key=lambda x: x[1])[-20:]], key= lambda x:x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers:\n",
    "* Why Am I targetting Transformers?\n",
    "    * Ans: I am trying to understand BERT: Bidirectional Embedding representation from Transformers.\n",
    "    \n",
    "* Why Am I reading about BERT ?\n",
    "    * Ans: To improve the performance of text classification task.\n",
    "    \n",
    "* So What is Transformers?\n",
    "    * Transformers is a type of neural network which combines Convolution Neural Network with Attention Models.\n",
    "    * They are built to sequence transduction or neural machine translation. \n",
    "    * Applications: text-to-speech,  speech recognition etc.\n",
    "    \n",
    "* What is Attention Models?\n",
    "\n",
    "* What is sequence transduction ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
