{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick notes on the concept which were not in Deep Learning modules of Machine Learning Nano Degree:\n",
    "\n",
    "##### Regarding DL for Text :\n",
    "* In text dataset, key terms are more important and we might no have enough dataset describing them. Deep Learning model usually expects many samples to understand the variance of a label. like image of a perticular flower.\n",
    "* Semantic relation between kitty and cat understanding also requires huge dataset for DL. If 2 words are similar then we have to share weights between them. i.e. the model should predict they are same when asked so the the internal weights of the model should indicate that the weights values are such that out of so huge NN (CNN) the trajectory of the word transformation for both the words are same.\n",
    "\n",
    "##### similar words occur in similar context:\n",
    "* As we have lot of text from the the wikipedia, we can have unsupervised learning to identify the similar words with **the assumption that similar words occur in similar context.** \n",
    "* This assumption helps in 2 ways :\n",
    "    * without knowing their meaning we can have associated words.\n",
    "    * representation of words can be done through the associated words rather than sparse matrix of n words with n words. This representation of words by associated word vector is called **embedding**. Clearly, this is smaller vector from the sparse matrix.\n",
    "    * with 1 word embedding we have multiple words association which are in the word embedding is obtained.\n",
    "    \n",
    "##### How do we get the word embedding?\n",
    "* **Word2Vec**: split a sentence it to 2 sets or words. One set of words trying to predict other set of words. Here simple logistic regression can be used to generate the model for prediction. The best set of word (also called window of words) that best represents the corpus (sentences, paragraph or book) is then saved.\n",
    "    * **skip grams**: predicts the neighboring context from a word\n",
    "    * **Continuous Bag Of Words**: predicts the word from neighboring context.\n",
    "    * https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
    "    * **Sampled Softmax**: When vocabulary of the target words are large, softmax may be in-efficient. Therefore, **non class targets** are randomly sampled and with this small subset used for training the model.\n",
    "\n",
    "* **t-SNE**: is a visualization technique where the association of words are projected onto 2D with relation between the words are retained. Closer words are plotted closer and loosely coupled words are plotted far away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### So, now, we have word embedding...what next ?\n",
    "* compare them using cosine similarity. Why compare? thats the most common activity on text. Applications like plagarism check, classifying documents based on authors require word embedding to be compared.\n",
    "\n",
    "\n",
    "* can it also help in Q&A ? predicting next sequence of sentences ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RNN: Recurring Neural Network\n",
    "* takes into consideration of input vector time. i.e. what was the state of input at t-1th instance. it shares weights across time rather than space as in CNN.\n",
    "* since input changes over time and at any point it depends on previous state, a model is used to summarized the events at past. This model forms the recurrent connetion.\n",
    "* A repeatable network of recurrent connection to summarize the past, input vector and predicting model (classification or regression) is called **RNN**. Usually the predicting model is same in the repeated network.\n",
    "* Here input vector remains the same but the tth second we would take into consider recursive transformation over t-1 and before.\n",
    "* **back propagation over time** would result in correlated update to weights which would result in bad gradient descent. Correlated update is because of recurrent network which has the wieght carried in the output of t-1 second.\n",
    "\n",
    "* **vanishing gradients**: here gradients abruptly decreases to zero. This would result no training to model. Also result in memory loss and model do not keep tracks of more distant past and keeps track of only recent past.\n",
    "\n",
    "* **exploding gradients**: here gradients abruptly increases to infinity. This can be addressed by Gradient clipping where gradients are normalized and when gradients are too huge the steps are cut shorted. i.e. when u start back propagating the point at which it reaches the max threshold u just stop increasing the gradients and the same value is assumed for t-x and less.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM: Long Short Term Memory\n",
    "* addresses the vanishing gradients problem in RNN\n",
    "* replaces the predicting model in RNN with LSTM \"cell\" which does the memory management. This memory management helps network to remember the older events. Thus solving vanishing gradient problem.\n",
    "* memory management operation involves storing, forgetting and reading memory.\n",
    "* instead of making those operation descrete result oriented, it is controlled to through logistic regression which makes it easier for partial read, store and delete.\n",
    "* **regularization**: L2 regularization can be used on all 4 sides (2 inputs and 2 outputs). Dropout can be used in only input and/or output and not between recurring network and connect future areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RNN Application : Beam Search\n",
    "* RNN are for time series activities prediction, one example is word prediction like that one Google search bar does.\n",
    "* It takes the input of so far entered text and predicts the upcoming word or characters.\n",
    "* Taking one prediction at a time would be too greedy, rather to take couple of them and continue the sequence for all of them.\n",
    "* this would mean that there are now multiple sequence being investigated \n",
    "* Now based on the final probabilities score, one sequence can be finalized\n",
    "* This approach is good to avoid scenario which accidentally landing to one word, but now we have couple sequences to choose from.\n",
    "\n",
    "e.g.: App can lead to both apple or append or application. Now just because one model chooses,say apple, because its probability is slightly larger than other you would loose option of other 2 words.\n",
    "\n",
    "If you had chosen both 'l' & 'e' and continued series then you would have at the end to choose from apple, append and application. Now based on the probability score (multiply all the predicted probabilities) you are in better position to choose the word.\n",
    "\n",
    "* Maintaining all sequences might not be cost effective(time and memory). For this we have Beam search.\n",
    "* it prunes the sequence which are not so likely and keeps only the most likely. It define a beam width which defines the most likely sequence window there by simulating beam like projection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM through Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46, 48], [40, 9], [32, 7], [40, 9], [4], [25], [39, 7], [22, 40], [39, 9], [41, 6, 48, 38]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46 48  0  0]\n",
      " [40  9  0  0]\n",
      " [32  7  0  0]\n",
      " [40  9  0  0]\n",
      " [ 4  0  0  0]\n",
      " [25  0  0  0]\n",
      " [39  7  0  0]\n",
      " [22 40  0  0]\n",
      " [39  9  0  0]\n",
      " [41  6 48 38]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\padmaraj.bhat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Embedding\n",
    "#from tensorflow.keras.embeddings import Embedding\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 100us/sample - loss: 0.2993 - acc: 1.0000\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82694954],\n",
       "       [0.77747256],\n",
       "       [0.7040919 ],\n",
       "       [0.77747256],\n",
       "       [0.6916161 ],\n",
       "       [0.28503722],\n",
       "       [0.29529685],\n",
       "       [0.23452783],\n",
       "       [0.42103335],\n",
       "       [0.07797129]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just with \"not\" what is the score ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27370414]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([22,0,0,0]).reshape(1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### it is near to score and hence negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just with \"great\", what is the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6872823]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([32,0,0,0]).reshape(1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment prediction is \"positive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just with \"work\", what is the score ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6872823]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([32,0,0,0]).reshape(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5001465]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([0,32,0,0]).reshape(1,4)) # work as 2nd word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30238178]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([22,32,0,0]).reshape(1,4)) # not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30804333]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([39,32,0,0]).reshape(1,4)) #poor work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* with \"work\" as first word it is predicting positive sentiment\n",
    "* with \"work\" as the second word it is predicting negative sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if there are words as labels?\n",
    "* Like quotes fo the sentence and author of the sentence as labels ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_str = [(\"A for\", \"Apple\")\n",
    "            , (\"B for\", \"Ball\")\n",
    "            , (\"C for\", \"cat\")\n",
    "            , (\"Kitty can also mean\", \"cat\")\n",
    "            , (\" meow sound can also mean\", \"cat\")\n",
    "            , (\"D for\", \"Dog\")\n",
    "            , (\"A is also for\", \"Ant\")\n",
    "            , (\"D can also have\", \"Drum\")\n",
    "            , (\"B is popularly known shortcut for\", \"Be\")\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* here a simple classifier can map the the sequence of text based on length or BoW can lead to a class. I guess this not a good example for LSTM.\n",
    "\n",
    "* Here if the model is formed it might predict for \"D is also know for\" as \"Ant\". Let us check !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24, 18], [19, 18], [90, 18], [27, 40, 28, 29], [31, 95, 40, 28, 29], [77, 18], [24, 43, 28, 18], [77, 40, 28, 46], [19, 43, 6, 14, 3, 18]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 100\n",
    "encoded_docs = [one_hot(d[0], vocab_size) for d in docs_str]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24 18  0  0  0  0]\n",
      " [19 18  0  0  0  0]\n",
      " [90 18  0  0  0  0]\n",
      " [27 40 28 29  0  0]\n",
      " [31 95 40 28 29  0]\n",
      " [77 18  0  0  0  0]\n",
      " [24 43 28 18  0  0]\n",
      " [77 40 28 46  0  0]\n",
      " [19 43  6 14  3 18]]\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(x) for x in encoded_docs])\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 6, 20)             2000      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 2,121\n",
      "Trainable params: 2,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 20, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 53, 78, 78, 78, 49, 85, 70, 23]\n",
      "9/9 [==============================] - 0s 111us/sample - loss: -708.7500 - acc: 0.0000e+00\n",
      "Accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "labels = [ one_hot(y, vocab_size)[0] for y in [x[1] for x in docs_str] ]\n",
    "print(labels)\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 6 6 6 4 0 5 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "labels = [x[1] for x in docs_str]\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\padmaraj.bhat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "onehot_encoded = onehot_encoder.fit_transform(np.array(labels).reshape(-1,1))\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 6, 30)             3000      \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 1267      \n",
      "=================================================================\n",
      "Total params: 4,267\n",
      "Trainable params: 4,267\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 30, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(7, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 111us/sample - loss: 0.2301 - acc: 1.0000\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, onehot_encoded, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, onehot_encoded, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01943946, 0.35496178, 0.04048574, 0.0042848 , 0.04294088,\n",
       "        0.00186032, 0.02373973],\n",
       "       [0.00214723, 0.03345937, 0.31550813, 0.0127781 , 0.01372948,\n",
       "        0.00122043, 0.06902653],\n",
       "       [0.00129768, 0.01842001, 0.05497   , 0.00307879, 0.04996714,\n",
       "        0.00225902, 0.33951503],\n",
       "       [0.00857687, 0.00259757, 0.00412455, 0.00419933, 0.00232127,\n",
       "        0.02179644, 0.5118555 ],\n",
       "       [0.00760907, 0.00786635, 0.00805849, 0.01714033, 0.00963524,\n",
       "        0.01276016, 0.90130043],\n",
       "       [0.00285327, 0.0481348 , 0.02228722, 0.00250304, 0.36661592,\n",
       "        0.01408225, 0.06488186],\n",
       "       [0.5527029 , 0.01783398, 0.00162145, 0.01724142, 0.00420105,\n",
       "        0.01095372, 0.01680672],\n",
       "       [0.02444848, 0.00358361, 0.00100103, 0.00916204, 0.01780856,\n",
       "        0.6778833 , 0.07911226],\n",
       "       [0.07609388, 0.01595449, 0.0753158 , 0.9118255 , 0.00963558,\n",
       "        0.03528861, 0.08005422]], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text : [77, 43, 28, 14, 18]\n",
      "[[77 43 28 14 18  0]]\n"
     ]
    }
   ],
   "source": [
    "text = \"D is also known for\"\n",
    "encoded_text = one_hot(text, vocab_size)\n",
    "print(\"Encoded text :\", encoded_text)\n",
    "padded_text = pad_sequences([encoded_text], maxlen=max_length, padding='post')\n",
    "print(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27758425, 0.00707912, 0.00233823, 0.08292675, 0.14962743,\n",
       "        0.32138762, 0.07658859]], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text : [77, 43, 14, 3, 18]\n",
      "[[77 43 14  3 18  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.18166593, 0.04393473, 0.01495257, 0.05598897, 0.3390646 ,\n",
       "        0.28406417, 0.21930079]], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"D is known shortcut for\"\n",
    "encoded_text = one_hot(text, vocab_size)\n",
    "print(\"Encoded text :\", encoded_text)\n",
    "padded_text = pad_sequences([encoded_text], maxlen=max_length, padding='post')\n",
    "print(padded_text)\n",
    "model.predict(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text : [9, 43, 14, 3, 18]\n",
      "[[ 9 43 14  3 18  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.22576067, 0.06020024, 0.04715523, 0.11328119, 0.05352072,\n",
       "        0.07511067, 0.26769105]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"F is known shortcut for\"\n",
    "encoded_text = one_hot(text, vocab_size)\n",
    "print(\"Encoded text :\", encoded_text)\n",
    "padded_text = pad_sequences([encoded_text], maxlen=max_length, padding='post')\n",
    "print(padded_text)\n",
    "model.predict(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text : [76, 48, 82, 3, 15, 43, 18]\n",
      "[[48 82  3 15 43 18]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.22029433, 0.3572051 , 0.27860332, 0.38095355, 0.30702597,\n",
       "        0.20268635, 0.33346674]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I dont know which one is this\"\n",
    "encoded_text = one_hot(text, vocab_size)\n",
    "print(\"Encoded text :\", encoded_text)\n",
    "padded_text = pad_sequences([encoded_text], maxlen=max_length, padding='post')\n",
    "print(padded_text)\n",
    "model.predict(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text : [86]\n",
      "[[86  0  0  0  0  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00523269, 0.03013298, 0.03118229, 0.00637817, 0.02094114,\n",
       "        0.00545558, 0.14406063]], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"what?\"\n",
    "encoded_text = one_hot(text, vocab_size)\n",
    "print(\"Encoded text :\", encoded_text)\n",
    "padded_text = pad_sequences([encoded_text], maxlen=max_length, padding='post')\n",
    "print(padded_text)\n",
    "model.predict(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text : [86, 43, 18]\n",
      "[[86 43 18  0  0  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03901008, 0.01958153, 0.02946073, 0.03354695, 0.03508037,\n",
       "        0.0135765 , 0.09712441]], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"what is this?\"\n",
    "encoded_text = one_hot(text, vocab_size)\n",
    "print(\"Encoded text :\", encoded_text)\n",
    "padded_text = pad_sequences([encoded_text], maxlen=max_length, padding='post')\n",
    "print(padded_text)\n",
    "model.predict(padded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Why  would we choose LSTM in this case? We could do it without that too right ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After an hours of search .... I could not get a precise example where it is absolutely necessary to use LSTM (or RNN).\n",
    "\n",
    "##### It is always claimed that it gave good results in text translation and music notes prediction and in games but I see it as below\n",
    "* why would not a simple which does the classification be used iteravatively to produce the same result as that of LSTM.\n",
    "    * e.g.: I can iteratively get the probability of the subsequent of characters from a trained model.\n",
    "* if memory is the advantage in LSTM, then why not a simple search give better result or at least same result as that of LSTM.\n",
    "    * e.g.: I can SEARCH the series of musical note and predict the next note. Here, I agree that the search time is much more than the prediction time of the LSTM but I guess we can always improvise search techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: Not really an answer. The outro to the lesson carefully make the same point above. That when we have enough data and computation power, we can mix various technique or more professionally indicated as \"layers or techniques\" or \"stacks of techniques\" to build a new model which *OFTEN* does better job that hand crafted approaches to the problem.\n",
    "\n",
    "* This indirectly tells me that we can have simplified approach and machine learning approach but machine learning can be fine tuned with various techniques like number of layers, number neurons, activation function etc. However, in naive approach it has to be industry or data scientist solving the problem with careful coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Models as Lego:\n",
    "* Deep neural network is nothing but hidden nueral network layers; now it is upto you how do you want to form layers. it can be RNN, LSTM and CNN combinations. \n",
    "* Why ?: e.g.: for a speech recognition you can optimize the speech recognition pattern  to complete the translation. so you may have cnn for the speech recognition but later lstm to understand the missing sequence pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick note on CNN and maxpool:\n",
    "- CNN is a filter to recognize the elements.\n",
    "- Max pool is to downsize the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Outlier:\n",
    "* we know that plotting is the best way to know the outlier but a isoloated point does not indicate it is an outlier\n",
    "    * e.g.: in fraud detection you are expected to have the fraud characterstic outside the normal\n",
    "    * in rent prediction higher sqft usually have has hight rent or saleprice\n",
    "        * even if the sqft is high based on the neighborhood house rent may be low for that area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
