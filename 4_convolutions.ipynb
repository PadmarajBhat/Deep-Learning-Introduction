{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IZYv70SvvOan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\padmaraj.bhat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.717376\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 8.9%\n",
      "Minibatch loss at step 50: 1.953475\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 45.9%\n",
      "Minibatch loss at step 100: 1.492779\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 150: 0.700998\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 200: 0.681988\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 250: 1.377413\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 300: 0.545901\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 350: 0.188070\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 400: 0.328839\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 450: 0.444017\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 500: 0.583691\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 550: 0.884941\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 600: 0.446474\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 650: 0.596101\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 700: 1.323900\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 750: 0.544892\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 800: 0.581785\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 850: 0.585640\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 900: 0.835487\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 950: 0.816153\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1000: 0.407563\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1050: 0.594749\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 1100: 0.324717\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 1150: 0.514985\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 1200: 0.727669\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 1250: 0.134143\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 1300: 0.406847\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 1350: 0.936268\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 1400: 0.338375\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 1450: 0.269906\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 1500: 0.932897\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 1550: 0.441363\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1600: 1.101242\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1650: 0.599538\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 1700: 0.934124\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 1750: 0.080869\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 1800: 0.036775\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 1850: 0.585072\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1900: 0.562606\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 1950: 0.879374\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2000: 0.577471\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 2050: 0.474816\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 2100: 0.185493\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 2150: 0.463058\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 2200: 0.308363\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 2250: 0.100381\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 2300: 0.890054\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 2350: 0.163316\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 2400: 0.471688\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 2450: 0.279636\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 2500: 0.867070\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 2550: 0.395448\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 2600: 0.640392\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 2650: 0.290701\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 2700: 0.717015\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 2750: 1.110194\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 2800: 0.638410\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 2850: 0.046312\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 2900: 0.100685\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 2950: 0.881966\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 3000: 0.759092\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.8%\n",
      "Test accuracy: 92.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, BatchNormalization, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\padmaraj.bhat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential([\n",
    "    Conv2D(filters= 32,kernel_size=(3,3), strides=1, padding='valid', activation='relu', input_shape=(28,28,1) ),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    Dropout(rate=0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(rate=0.25),\n",
    "    Dense(10, activation='softmax'),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "model_1.compile( optimizer='adam'\n",
    "              , loss='sparse_categorical_crossentropy'\n",
    "              , metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "\n",
    "model_2 = Sequential([\n",
    "    Conv2D(filters= 32,kernel_size=(3,3), strides=1, padding='valid', activation='relu', input_shape=(28,28,1) ),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    Dropout(rate=0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(rate=0.25),\n",
    "    Dense(10, activation='softmax'),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "model_2.compile( optimizer='adam'\n",
    "              , loss='sparse_categorical_crossentropy'\n",
    "              , metrics=['accuracy']\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_dataset_shape = train_dataset.shape\n",
    "train_dataset = train_dataset.reshape((train_dataset_shape[0], train_dataset_shape[1], train_dataset_shape[2], 1))\n",
    "valid_shape = valid_dataset.shape\n",
    "valid_dataset = valid_dataset.reshape((valid_shape[0], valid_shape[1],valid_shape[2],1))\n",
    "test_shape = test_dataset.shape\n",
    "test_dataset = test_dataset.reshape((test_shape[0], test_shape[1],test_shape[2],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_generator(batch_size):\n",
    "    for batch_index in np.random.choice(range(train_dataset.shape[0])\n",
    "                                        ,size=(train_dataset.shape[0]//batch_size, batch_size)\n",
    "                                        ,replace=False\n",
    "                                       ):\n",
    "        yield train_dataset[batch_index], train_labels[batch_index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "200000/200000 [==============================] - 307s 2ms/sample - loss: 0.4302 - acc: 0.8725 - val_loss: 0.3377 - val_acc: 0.8990\n",
      "Epoch 2/5\n",
      "200000/200000 [==============================] - 280s 1ms/sample - loss: 0.3107 - acc: 0.9068 - val_loss: 0.2929 - val_acc: 0.9131\n",
      "Epoch 3/5\n",
      "200000/200000 [==============================] - 286s 1ms/sample - loss: 0.2640 - acc: 0.9197 - val_loss: 0.2766 - val_acc: 0.9166\n",
      "Epoch 4/5\n",
      "200000/200000 [==============================] - 301s 2ms/sample - loss: 0.2250 - acc: 0.9302 - val_loss: 0.2659 - val_acc: 0.9217\n",
      "Epoch 5/5\n",
      "200000/200000 [==============================] - 265s 1ms/sample - loss: 0.1901 - acc: 0.9412 - val_loss: 0.2667 - val_acc: 0.9237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dd9ddf4a20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(  x=train_dataset\n",
    "          , y=train_labels\n",
    "          , batch_size=batch_size\n",
    "          , epochs=5\n",
    "          , validation_data=(valid_dataset,valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 477us/sample - loss: 0.1099 - acc: 0.9701s - loss: 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10986645233016461, 0.9701]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(test_dataset,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 0.4365 - acc: 0.8712\n",
      "312/312 [==============================] - 79s 252ms/step - loss: 0.5860 - acc: 0.8288 - val_loss: 0.4357 - val_acc: 0.8712\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 3s 298us/sample - loss: 0.3815 - acc: 0.8843\n",
      "312/312 [==============================] - 72s 232ms/step - loss: 0.4227 - acc: 0.8746 - val_loss: 0.3812 - val_acc: 0.8843\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 3s 323us/sample - loss: 0.3731 - acc: 0.8864\n",
      "312/312 [==============================] - 65s 209ms/step - loss: 0.3891 - acc: 0.8847 - val_loss: 0.3715 - val_acc: 0.8864\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 2s 244us/sample - loss: 0.3361 - acc: 0.8987\n",
      "312/312 [==============================] - 72s 230ms/step - loss: 0.3729 - acc: 0.8884 - val_loss: 0.3351 - val_acc: 0.8987\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 4s 358us/sample - loss: 0.3204 - acc: 0.9031\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 0.3516 - acc: 0.8957 - val_loss: 0.3205 - val_acc: 0.9031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ddab5265f8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit_generator(my_generator(batch_size)\n",
    "                    , train_dataset.shape[0]//batch_size//5\n",
    "                    , epochs=5\n",
    "                , validation_data=(valid_dataset,valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 495us/sample - loss: 0.1548 - acc: 0.9545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1547955856844783, 0.9545]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.evaluate(test_dataset,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_3 = Sequential([\n",
    "    Conv2D(filters= 32,kernel_size=(3,3), strides=1, padding='valid', activation='relu', input_shape=(28,28,1) ),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    Dropout(rate=0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(rate=0.25),\n",
    "    Dense(10, activation='softmax'),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "model_3.compile( optimizer='adam'\n",
    "              , loss='sparse_categorical_crossentropy'\n",
    "              , metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 4s 356us/sample - loss: 0.4461 - acc: 0.8681\n",
      "312/312 [==============================] - 73s 235ms/step - loss: 0.6044 - acc: 0.8259 - val_loss: 0.4436 - val_acc: 0.8681\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 3s 276us/sample - loss: 0.3822 - acc: 0.8864\n",
      "312/312 [==============================] - 69s 222ms/step - loss: 0.4322 - acc: 0.8715 - val_loss: 0.3809 - val_acc: 0.8864\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 3s 277us/sample - loss: 0.3541 - acc: 0.8945\n",
      "312/312 [==============================] - 67s 214ms/step - loss: 0.3861 - acc: 0.8853 - val_loss: 0.3535 - val_acc: 0.8945\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 3s 256us/sample - loss: 0.3437 - acc: 0.8971\n",
      "312/312 [==============================] - 65s 210ms/step - loss: 0.3692 - acc: 0.8893 - val_loss: 0.3438 - val_acc: 0.8971\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 2s 234us/sample - loss: 0.3274 - acc: 0.9026\n",
      "312/312 [==============================] - 60s 193ms/step - loss: 0.3564 - acc: 0.8945 - val_loss: 0.3266 - val_acc: 0.9026\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 2s 226us/sample - loss: 0.3175 - acc: 0.9069\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 0.3138 - acc: 0.9051 - val_loss: 0.3167 - val_acc: 0.9069\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 2s 246us/sample - loss: 0.3126 - acc: 0.9073\n",
      "312/312 [==============================] - 55s 175ms/step - loss: 0.3140 - acc: 0.9052 - val_loss: 0.3106 - val_acc: 0.9073\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.3008 - acc: 0.9097s - loss: 0.3073 - ac\n",
      "312/312 [==============================] - 62s 200ms/step - loss: 0.3129 - acc: 0.9072 - val_loss: 0.2986 - val_acc: 0.9097\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 2s 250us/sample - loss: 0.2978 - acc: 0.9115\n",
      "312/312 [==============================] - 56s 180ms/step - loss: 0.3052 - acc: 0.9088 - val_loss: 0.2962 - val_acc: 0.9115\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 2s 231us/sample - loss: 0.2954 - acc: 0.9108\n",
      "312/312 [==============================] - 63s 203ms/step - loss: 0.3024 - acc: 0.9093 - val_loss: 0.2943 - val_acc: 0.9108\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    model_3.fit_generator(my_generator(batch_size)\n",
    "                    , train_dataset.shape[0]//batch_size//5\n",
    "                    , epochs=5\n",
    "                , validation_data=(valid_dataset,valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 335us/sample - loss: 0.1321 - acc: 0.9627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13205413690134882, 0.9627]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.evaluate(test_dataset,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let us give 3 more rounds to mode_3 which would logically equivalent to the way traying was done for model_1.\n",
    "i.e. training entire training dataset 5 times through 5 epochs  == 5 times full batches; with one full set of batches through 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 3s 265us/sample - loss: 0.2867 - acc: 0.9138s \n",
      "312/312 [==============================] - 64s 205ms/step - loss: 0.2616 - acc: 0.9208 - val_loss: 0.2859 - val_acc: 0.9138\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 3s 255us/sample - loss: 0.2877 - acc: 0.9149\n",
      "312/312 [==============================] - 61s 197ms/step - loss: 0.2534 - acc: 0.9220 - val_loss: 0.2859 - val_acc: 0.9149\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 3s 273us/sample - loss: 0.2820 - acc: 0.9151\n",
      "312/312 [==============================] - 59s 189ms/step - loss: 0.2644 - acc: 0.9194 - val_loss: 0.2805 - val_acc: 0.9151\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 0.2823 - acc: 0.9146\n",
      "312/312 [==============================] - 59s 190ms/step - loss: 0.2714 - acc: 0.9173 - val_loss: 0.2824 - val_acc: 0.9146\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 3s 259us/sample - loss: 0.2743 - acc: 0.9179\n",
      "312/312 [==============================] - 58s 187ms/step - loss: 0.2596 - acc: 0.9211 - val_loss: 0.2725 - val_acc: 0.9179\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 2s 208us/sample - loss: 0.2765 - acc: 0.9192\n",
      "312/312 [==============================] - 66s 211ms/step - loss: 0.2128 - acc: 0.9336 - val_loss: 0.2744 - val_acc: 0.9192\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 2s 222us/sample - loss: 0.2789 - acc: 0.9175\n",
      "312/312 [==============================] - 58s 187ms/step - loss: 0.2221 - acc: 0.9309 - val_loss: 0.2769 - val_acc: 0.9175\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 2s 233us/sample - loss: 0.2735 - acc: 0.9214\n",
      "312/312 [==============================] - 55s 175ms/step - loss: 0.2240 - acc: 0.9311 - val_loss: 0.2714 - val_acc: 0.9214\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 3s 295us/sample - loss: 0.2666 - acc: 0.9210\n",
      "312/312 [==============================] - 59s 189ms/step - loss: 0.2311 - acc: 0.9291 - val_loss: 0.2643 - val_acc: 0.9210\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 3s 266us/sample - loss: 0.2719 - acc: 0.9208\n",
      "312/312 [==============================] - 60s 192ms/step - loss: 0.2329 - acc: 0.9277 - val_loss: 0.2696 - val_acc: 0.9208\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    model_3.fit_generator(my_generator(batch_size)\n",
    "                    , train_dataset.shape[0]//batch_size//5\n",
    "                    , epochs=5\n",
    "                , validation_data=(valid_dataset,valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 342us/sample - loss: 0.1078 - acc: 0.9692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10775131025463343, 0.9692]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.evaluate(test_dataset,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is mere conincidental that batches gave early orientation to the final result. We just saw the snapshot of the entire dataset training. even model1 would have same result at the same snapshot. \n",
    "\n",
    "* We should make sure that each batch has balanced labels. We know for sure we have balanced dataset; we should extend the same in each batches.\n",
    "\n",
    "* If possible we should also try find the outlier and try a technique by name batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 20000,\n",
       "         9: 20000,\n",
       "         6: 20000,\n",
       "         2: 20000,\n",
       "         7: 20000,\n",
       "         3: 20000,\n",
       "         5: 20000,\n",
       "         0: 20000,\n",
       "         1: 20000,\n",
       "         8: 20000})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 123 102.4 7.592101158440923\n",
      "85 123 102.4 11.85073837362044\n",
      "86 117 102.4 9.404254356406998\n",
      "83 123 102.4 11.783038657324349\n",
      "88 115 102.4 8.114185110040317\n",
      "92 113 102.4 6.57571288910944\n",
      "83 119 102.4 10.864621484432856\n",
      "84 118 102.4 10.170545708072895\n",
      "87 114 102.4 7.512655988397179\n",
      "93 114 102.4 6.814690014960329\n",
      "90 115 102.4 7.310266752998826\n",
      "94 114 102.4 6.931089380465383\n",
      "81 118 102.4 11.271202242884296\n",
      "87 112 102.4 6.988562083862459\n",
      "85 116 102.4 8.924124606929242\n",
      "77 124 102.4 13.580868897091968\n",
      "92 116 102.4 7.83836717690617\n",
      "84 126 102.4 10.82774214691133\n",
      "96 110 102.4 4.363484845854286\n",
      "81 118 102.4 10.278132126023678\n",
      "88 121 102.4 9.210863151735564\n",
      "85 120 102.4 10.509043724335722\n",
      "86 118 102.4 10.827742146911332\n",
      "84 119 102.4 10.150862032359617\n",
      "84 110 102.4 7.2\n",
      "91 111 102.4 5.9194594347794975\n",
      "83 118 102.4 10.44222198576529\n",
      "89 118 102.4 9.86103442849684\n",
      "86 113 102.4 8.42852300228219\n",
      "90 122 102.4 10.041912168506554\n",
      "86 124 102.4 12.109500402576483\n",
      "88 117 102.4 8.321057625085889\n",
      "84 114 102.4 9.046546302318914\n",
      "83 122 102.4 13.116401945655676\n",
      "91 113 102.4 6.606057825965498\n",
      "86 117 102.4 10.50904372433572\n",
      "76 116 102.4 10.489995233554684\n",
      "85 118 102.4 11.612062693595828\n",
      "90 115 102.4 7.722693830523129\n",
      "87 113 102.4 8.2365041127896\n",
      "93 119 102.4 8.475848040166836\n",
      "94 114 102.4 5.5533773507659285\n",
      "87 117 102.4 9.340235543068493\n",
      "91 121 102.4 9.43610088966836\n",
      "83 133 102.4 13.222707740852476\n",
      "93 114 102.4 7.28285658241325\n",
      "89 116 102.4 9.243376006633074\n",
      "87 113 102.4 8.416650165000325\n",
      "84 119 102.4 9.243376006633074\n",
      "82 117 102.4 10.190191362285598\n",
      "85 114 102.4 9.488940931421167\n",
      "82 121 102.4 10.355674772799695\n",
      "85 115 102.4 10.11137972781163\n",
      "93 111 102.4 5.642694391866355\n",
      "90 119 102.4 8.440379138403676\n",
      "92 113 102.4 7.418894796396564\n",
      "90 120 102.4 9.046546302318914\n",
      "87 124 102.4 11.680753400359071\n",
      "90 116 102.4 8.8\n",
      "88 119 102.4 9.871170143402452\n",
      "87 117 102.4 10.394229168149026\n",
      "87 123 102.4 12.93213052826177\n",
      "90 114 102.4 7.552483035399682\n",
      "88 117 102.4 8.8\n",
      "77 113 102.4 9.457272334029511\n",
      "84 118 102.4 8.822698000045111\n",
      "86 112 102.4 8.845337754998392\n",
      "88 118 102.4 8.114185110040317\n",
      "83 114 102.4 10.061808982484214\n",
      "88 125 102.4 11.29778739399888\n",
      "79 129 102.4 14.520330574749323\n",
      "84 118 102.4 11.271202242884296\n",
      "90 116 102.4 10.403845442911962\n",
      "91 120 102.4 8.404760555780278\n",
      "88 116 102.4 8.534635317340747\n",
      "90 115 102.4 8.126499861564017\n",
      "88 127 102.4 10.799999999999999\n",
      "92 109 102.4 6.135144660071187\n",
      "91 122 102.4 9.00222194794152\n",
      "81 120 102.4 12.167168939404105\n",
      "87 122 102.4 10.716342659695052\n",
      "88 121 102.4 11.074294559925702\n",
      "92 116 102.4 7.2\n",
      "92 116 102.4 8.56971411425142\n",
      "85 123 102.4 9.8812954616285\n",
      "92 120 102.4 9.820386957752733\n",
      "84 119 102.4 9.16733330909267\n",
      "93 116 102.4 6.6663333249995835\n",
      "90 120 102.4 9.046546302318914\n",
      "89 117 102.4 8.534635317340747\n",
      "90 113 102.4 7.445804187594514\n",
      "91 121 102.4 10.6226173799116\n",
      "90 131 102.4 11.324310133513652\n",
      "91 117 102.4 8.357032966310472\n",
      "81 128 102.4 14.086873322352266\n",
      "84 121 102.4 10.480458005259122\n",
      "85 118 102.4 9.436100889668358\n",
      "94 116 102.4 7.310266752998827\n",
      "86 118 102.4 10.355674772799695\n",
      "87 115 102.4 6.974238309665077\n",
      "91 121 102.4 10.00199980003999\n",
      "82 125 102.4 12.571396103854177\n",
      "93 113 102.4 5.817215828899595\n",
      "85 124 102.4 10.55651457631732\n",
      "87 114 102.4 8.593020423576334\n",
      "86 122 102.4 10.209799214480176\n",
      "82 114 102.4 7.927168473042565\n",
      "89 125 102.4 9.687104830649869\n",
      "88 124 102.4 11.306635220082056\n",
      "84 123 102.4 11.680753400359071\n",
      "88 118 102.4 9.991996797437437\n",
      "87 119 102.4 11.2\n",
      "93 113 102.4 6.666333324999584\n",
      "90 111 102.4 6.8585712797928995\n",
      "81 113 102.4 8.742997197757758\n",
      "90 108 102.4 5.4626001134990645\n",
      "90 125 102.4 9.911609354691093\n",
      "83 116 102.4 9.8812954616285\n",
      "90 112 102.4 6.421837743200929\n",
      "94 112 102.4 6.102458520956944\n",
      "85 122 102.4 9.840731680114036\n",
      "89 120 102.4 8.558037158133867\n",
      "88 118 102.4 9.531002045955084\n",
      "91 121 102.4 9.718024490605073\n",
      "88 115 102.4 7.901898506055364\n",
      "85 126 102.4 13.169662106523463\n",
      "93 115 102.4 6.248199740725323\n",
      "89 111 102.4 7.472616676907762\n",
      "88 114 102.4 7.618398781896364\n",
      "83 119 102.4 11.412274094149685\n",
      "92 117 102.4 6.681317235396026\n",
      "89 124 102.4 9.156418513807678\n",
      "86 123 102.4 13.177253128023306\n",
      "84 122 102.4 11.672189169131899\n",
      "91 115 102.4 7.017121917139533\n",
      "88 119 102.4 9.614572273377533\n",
      "92 119 102.4 7.800000000000001\n",
      "85 117 102.4 10.974515934655159\n",
      "88 116 102.4 9.2\n",
      "75 118 102.4 13.10877568653915\n",
      "90 115 102.4 8.416650165000325\n",
      "84 117 102.4 10.725670142233538\n",
      "92 118 102.4 8.404760555780278\n",
      "91 109 102.4 5.76541412215983\n",
      "84 115 102.4 9.820386957752735\n",
      "82 123 102.4 10.716342659695052\n",
      "81 119 102.4 10.584894897919394\n",
      "95 116 102.4 5.607138307550475\n",
      "90 123 102.4 8.946507698538015\n",
      "83 120 102.4 9.73858305915188\n",
      "94 121 102.4 8.440379138403676\n",
      "85 124 102.4 10.846197490365\n",
      "91 113 102.4 7.405403432629447\n",
      "91 116 102.4 8.089499366462674\n",
      "85 114 102.4 7.9899937421752725\n",
      "90 115 102.4 6.590902821313632\n",
      "90 113 102.4 6.437390775772433\n",
      "80 118 102.4 12.579348154813111\n",
      "93 120 102.4 8.2365041127896\n",
      "84 131 102.4 12.34665946724052\n",
      "88 114 102.4 9.830564581955606\n",
      "89 116 102.4 8.73155198117723\n",
      "80 120 102.4 11.80847153530041\n",
      "87 126 102.4 10.239140588936163\n",
      "88 117 102.4 7.282856582413249\n",
      "69 112 102.4 12.658593918757328\n",
      "88 118 102.4 9.830564581955606\n",
      "94 116 102.4 5.782732917920384\n",
      "88 116 102.4 8.126499861564017\n",
      "84 129 102.4 13.047605144240073\n",
      "91 122 102.4 8.46404158779953\n",
      "91 118 102.4 8.392854103342914\n",
      "94 116 102.4 6.2\n",
      "84 122 102.4 10.883014288330234\n",
      "87 119 102.4 9.38296328459192\n",
      "84 129 102.4 13.131641176943573\n",
      "79 129 102.4 14.853955702101715\n",
      "86 120 102.4 10.3072789813801\n",
      "88 118 102.4 9.264987857520376\n",
      "95 117 102.4 6.216108107168021\n",
      "92 111 102.4 6.311893535223801\n",
      "89 116 102.4 9.046546302318914\n",
      "91 116 102.4 7.914543574963751\n",
      "91 115 102.4 7.002856560004639\n",
      "90 114 102.4 8.546344247688598\n",
      "92 111 102.4 5.571355310873648\n",
      "87 119 102.4 8.662563131083086\n",
      "92 124 102.4 9.372299611087985\n",
      "88 110 102.4 6.78527818147495\n",
      "85 124 102.4 11.77454882362802\n",
      "90 115 102.4 7.8\n",
      "89 113 102.4 6.916646586316233\n",
      "93 113 102.4 7.405403432629448\n",
      "85 125 102.4 11.80847153530041\n",
      "88 116 102.4 7.709734106958553\n"
     ]
    }
   ],
   "source": [
    "for td, tl in my_generator(1024):\n",
    "    cntr_dict = dict(Counter(tl))\n",
    "    \n",
    "    print(np.array(list(cntr_dict.values())).min(),\n",
    "          np.array(list(cntr_dict.values())).max(),\n",
    "        np.array(list(cntr_dict.values())).mean(),\n",
    "        np.array(list(cntr_dict.values())).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is almost 1-6% deviation from the mean of the number of samples for a label. Hence we need not have to investigate much in this aspect of balanced batches when we have 10K sample/batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_4 = Sequential([\n",
    "    Conv2D(filters= 32,kernel_size=(3,3), strides=1, padding='valid', activation='relu', input_shape=(28,28,1) ),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    Dropout(rate=0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(rate=0.25),\n",
    "    Dense(10, activation='softmax'),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "model_4.compile( optimizer='adam'\n",
    "              , loss='sparse_categorical_crossentropy'\n",
    "              , metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "200000/200000 [==============================] - 177s 884us/sample - loss: 0.9746 - acc: 0.7132 - val_loss: 0.6466 - val_acc: 0.8241\n",
      "Epoch 2/5\n",
      "200000/200000 [==============================] - 167s 837us/sample - loss: 0.6018 - acc: 0.8311 - val_loss: 0.5339 - val_acc: 0.8480\n",
      "Epoch 3/5\n",
      "200000/200000 [==============================] - 152s 759us/sample - loss: 0.5115 - acc: 0.8536 - val_loss: 0.4722 - val_acc: 0.8648\n",
      "Epoch 4/5\n",
      "200000/200000 [==============================] - 158s 791us/sample - loss: 0.4600 - acc: 0.8672 - val_loss: 0.4325 - val_acc: 0.8725\n",
      "Epoch 5/5\n",
      "200000/200000 [==============================] - 164s 822us/sample - loss: 0.4248 - acc: 0.8766 - val_loss: 0.4033 - val_acc: 0.8813\n",
      "10000/10000 [==============================] - 3s 318us/sample - loss: 0.2137 - acc: 0.9406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21372416182756424, 0.9406]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4.fit(  x=train_dataset\n",
    "          , y=train_labels\n",
    "          , batch_size=10240\n",
    "          , epochs=5\n",
    "          , validation_data=(valid_dataset,valid_labels))\n",
    "model_4.evaluate(test_dataset,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "* Very High (200000) batchsize resulted in dead python kernel \n",
    "* High (100000) batchsize resulted in laptop getting hung\n",
    "*  moderately high (10240) resulted in low accuracy; this may be the fact that when there are too many X then specific features of a label might be lost/generalised thus reduction in loss does not occur.\n",
    "* I need to 2 more reading on 5120, 2560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "200000/200000 [==============================] - 147s 734us/sample - loss: 0.7983 - acc: 0.7736 - val_loss: 0.5592 - val_acc: 0.8440\n",
      "Epoch 2/5\n",
      "200000/200000 [==============================] - 157s 787us/sample - loss: 0.5057 - acc: 0.8556 - val_loss: 0.4513 - val_acc: 0.8687\n",
      "Epoch 3/5\n",
      "200000/200000 [==============================] - 169s 843us/sample - loss: 0.4290 - acc: 0.8748 - val_loss: 0.4011 - val_acc: 0.8795\n",
      "Epoch 4/5\n",
      "200000/200000 [==============================] - 163s 817us/sample - loss: 0.3890 - acc: 0.8863 - val_loss: 0.3667 - val_acc: 0.8923\n",
      "Epoch 5/5\n",
      "200000/200000 [==============================] - 161s 805us/sample - loss: 0.3601 - acc: 0.8942 - val_loss: 0.3479 - val_acc: 0.8952\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.1765 - acc: 0.9513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1764532840460539, 0.9513]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_5 = Sequential([\n",
    "    Conv2D(filters= 32,kernel_size=(3,3), strides=1, padding='valid', activation='relu', input_shape=(28,28,1) ),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    Dropout(rate=0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(rate=0.25),\n",
    "    Dense(10, activation='softmax'),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "model_5.compile( optimizer='adam'\n",
    "              , loss='sparse_categorical_crossentropy'\n",
    "              , metrics=['accuracy']\n",
    "             )\n",
    "model_5.fit(  x=train_dataset\n",
    "          , y=train_labels\n",
    "          , batch_size=5120\n",
    "          , epochs=5\n",
    "          , validation_data=(valid_dataset,valid_labels))\n",
    "model_5.evaluate(test_dataset,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "200000/200000 [==============================] - 169s 847us/sample - loss: 0.6346 - acc: 0.8195 - val_loss: 0.4538 - val_acc: 0.8677\n",
      "Epoch 2/5\n",
      "200000/200000 [==============================] - 145s 727us/sample - loss: 0.4114 - acc: 0.8799 - val_loss: 0.3717 - val_acc: 0.8897\n",
      "Epoch 3/5\n",
      "200000/200000 [==============================] - 148s 738us/sample - loss: 0.3549 - acc: 0.8961 - val_loss: 0.3399 - val_acc: 0.8978\n",
      "Epoch 4/5\n",
      "200000/200000 [==============================] - 160s 799us/sample - loss: 0.3224 - acc: 0.9047 - val_loss: 0.3208 - val_acc: 0.9039\n",
      "Epoch 5/5\n",
      "200000/200000 [==============================] - 145s 723us/sample - loss: 0.2948 - acc: 0.9126 - val_loss: 0.2956 - val_acc: 0.9133\n",
      "10000/10000 [==============================] - 4s 411us/sample - loss: 0.1325 - acc: 0.9633s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13245965872108936, 0.9633]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_6 = Sequential([\n",
    "    Conv2D(filters= 32,kernel_size=(3,3), strides=1, padding='valid', activation='relu', input_shape=(28,28,1) ),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    Dropout(rate=0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(rate=0.25),\n",
    "    Dense(10, activation='softmax'),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "model_6.compile( optimizer='adam'\n",
    "              , loss='sparse_categorical_crossentropy'\n",
    "              , metrics=['accuracy']\n",
    "             )\n",
    "model_6.fit(  x=train_dataset\n",
    "          , y=train_labels\n",
    "          , batch_size=2048\n",
    "          , epochs=5\n",
    "          , validation_data=(valid_dataset,valid_labels))\n",
    "model_6.evaluate(test_dataset,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let us now introduce Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From C:\\Users\\padmaraj.bhat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "200000/200000 [==============================] - 152s 758us/sample - loss: 0.4721 - acc: 0.8658 - val_loss: 0.5484 - val_acc: 0.8994\n",
      "Epoch 2/5\n",
      "200000/200000 [==============================] - 142s 708us/sample - loss: 0.3175 - acc: 0.9061 - val_loss: 0.3116 - val_acc: 0.9120\n",
      "Epoch 3/5\n",
      "200000/200000 [==============================] - 142s 711us/sample - loss: 0.2710 - acc: 0.9199 - val_loss: 0.2864 - val_acc: 0.9155\n",
      "Epoch 4/5\n",
      "200000/200000 [==============================] - 144s 720us/sample - loss: 0.2352 - acc: 0.9298 - val_loss: 0.2777 - val_acc: 0.9183\n",
      "Epoch 5/5\n",
      "200000/200000 [==============================] - 143s 714us/sample - loss: 0.2037 - acc: 0.9388 - val_loss: 0.2736 - val_acc: 0.9207\n",
      "10000/10000 [==============================] - 4s 362us/sample - loss: 0.1141 - acc: 0.9675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11411828586272896, 0.9675]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_7 = Sequential([\n",
    "    Conv2D(filters= 32,kernel_size=(3,3), strides=1, padding='valid', activation='relu', input_shape=(28,28,1) ),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    Dropout(rate=0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1024, activation=None),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    \n",
    "    Dropout(rate=0.25),\n",
    "    \n",
    "    Dense(10, activation=None),\n",
    "    #BatchNormalization(),\n",
    "    Activation('softmax'),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "model_7.compile( optimizer='adam'\n",
    "              , loss='sparse_categorical_crossentropy'\n",
    "              , metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "model_7.fit(  x=train_dataset\n",
    "          , y=train_labels\n",
    "          , batch_size=1024\n",
    "          , epochs=5\n",
    "          , validation_data=(valid_dataset,valid_labels))\n",
    "model_7.evaluate(test_dataset,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### It indicates that we can have various archictecture which would result in better result. I am stopping my analysis here because anyways I know that there are overlaps in training, validataion and testing dataset. So I will address the data wrangling first and then try out various architecture. Otherwise, the test accuracy is not reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
