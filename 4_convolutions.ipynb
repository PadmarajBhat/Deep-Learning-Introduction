{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IZYv70SvvOan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\padmaraj.bhat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.747132\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 9.6%\n",
      "Minibatch loss at step 50: 1.673349\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 57.5%\n",
      "Minibatch loss at step 100: 1.073132\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 68.9%\n",
      "Test accuracy: 75.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\padmaraj.bhat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential([\n",
    "    Conv2D(filters= 32,kernel_size=(3,3), strides=1, padding='valid', activation='relu', input_shape=(28,28,1) ),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    Dropout(rate=0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(rate=0.25),\n",
    "    Dense(10, activation='softmax'),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "model_1.compile( optimizer='adam'\n",
    "              , loss='sparse_categorical_crossentropy'\n",
    "              , metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "\n",
    "model_2 = Sequential([\n",
    "    Conv2D(filters= 32,kernel_size=(3,3), strides=1, padding='valid', activation='relu', input_shape=(28,28,1) ),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    Dropout(rate=0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(rate=0.25),\n",
    "    Dense(10, activation='softmax'),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "model_2.compile( optimizer='adam'\n",
    "              , loss='sparse_categorical_crossentropy'\n",
    "              , metrics=['accuracy']\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_dataset_shape = train_dataset.shape\n",
    "train_dataset = train_dataset.reshape((train_dataset_shape[0], train_dataset_shape[1], train_dataset_shape[2], 1))\n",
    "valid_shape = valid_dataset.shape\n",
    "valid_dataset = valid_dataset.reshape((valid_shape[0], valid_shape[1],valid_shape[2],1))\n",
    "test_shape = test_dataset.shape\n",
    "test_dataset = test_dataset.reshape((test_shape[0], test_shape[1],test_shape[2],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_generator():\n",
    "    for batch_index in np.random.choice(range(train_dataset.shape[0])\n",
    "                                        ,size=(train_dataset.shape[0]//batch_size, batch_size)\n",
    "                                        ,replace=False\n",
    "                                       ):\n",
    "        yield train_dataset[batch_index], train_labels[batch_index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "200000/200000 [==============================] - 264s 1ms/sample - loss: 0.4301 - acc: 0.8731 - val_loss: 0.3213 - val_acc: 0.9036\n",
      "Epoch 2/5\n",
      "200000/200000 [==============================] - 254s 1ms/sample - loss: 0.3108 - acc: 0.9067 - val_loss: 0.2890 - val_acc: 0.9145\n",
      "Epoch 3/5\n",
      "200000/200000 [==============================] - 252s 1ms/sample - loss: 0.2640 - acc: 0.9192 - val_loss: 0.2751 - val_acc: 0.9192\n",
      "Epoch 4/5\n",
      "200000/200000 [==============================] - 250s 1ms/sample - loss: 0.2243 - acc: 0.9311 - val_loss: 0.2672 - val_acc: 0.9197\n",
      "Epoch 5/5\n",
      "200000/200000 [==============================] - 249s 1ms/sample - loss: 0.1892 - acc: 0.9406 - val_loss: 0.2692 - val_acc: 0.9229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x295026748d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(  x=train_dataset\n",
    "          , y=train_labels\n",
    "          , batch_size=batch_size\n",
    "          , epochs=5\n",
    "          , validation_data=(valid_dataset,valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 439us/sample - loss: 0.1053 - acc: 0.9695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10526585588380695, 0.9695]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(test_dataset,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 2s 250us/sample - loss: 0.4319 - acc: 0.8695\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 0.5870 - acc: 0.8287 - val_loss: 0.4310 - val_acc: 0.8695\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 2s 232us/sample - loss: 0.3857 - acc: 0.8840\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 0.4223 - acc: 0.8742 - val_loss: 0.3861 - val_acc: 0.8840\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 2s 226us/sample - loss: 0.3536 - acc: 0.8933\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 0.3923 - acc: 0.8833 - val_loss: 0.3530 - val_acc: 0.8933\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 2s 226us/sample - loss: 0.3420 - acc: 0.8964\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 0.3711 - acc: 0.8884 - val_loss: 0.3402 - val_acc: 0.8964\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 2s 228us/sample - loss: 0.3246 - acc: 0.9034\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 0.3505 - acc: 0.8962 - val_loss: 0.3233 - val_acc: 0.9034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29508789400>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit_generator(my_generator()\n",
    "                    , train_dataset.shape[0]//batch_size//5\n",
    "                    , epochs=5\n",
    "                , validation_data=(valid_dataset,valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 425us/sample - loss: 0.1581 - acc: 0.9554s - l\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15810049138963223, 0.9554]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.evaluate(test_dataset,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_3 = Sequential([\n",
    "    Conv2D(filters= 32,kernel_size=(3,3), strides=1, padding='valid', activation='relu', input_shape=(28,28,1) ),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    Dropout(rate=0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(rate=0.25),\n",
    "    Dense(10, activation='softmax'),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "model_3.compile( optimizer='adam'\n",
    "              , loss='sparse_categorical_crossentropy'\n",
    "              , metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 3s 286us/sample - loss: 0.4440 - acc: 0.8659\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 0.5985 - acc: 0.8266 - val_loss: 0.4421 - val_acc: 0.8659\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 3s 250us/sample - loss: 0.3837 - acc: 0.8851\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 0.4341 - acc: 0.8726 - val_loss: 0.3839 - val_acc: 0.8851\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 3s 260us/sample - loss: 0.3646 - acc: 0.8899\n",
      "312/312 [==============================] - 58s 184ms/step - loss: 0.3949 - acc: 0.8836 - val_loss: 0.3643 - val_acc: 0.8899\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 2s 238us/sample - loss: 0.3393 - acc: 0.8977\n",
      "312/312 [==============================] - 57s 184ms/step - loss: 0.3733 - acc: 0.8884 - val_loss: 0.3382 - val_acc: 0.8977\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 2s 241us/sample - loss: 0.3255 - acc: 0.9012s - loss: 0.3263 - acc: \n",
      "312/312 [==============================] - 58s 186ms/step - loss: 0.3527 - acc: 0.8933 - val_loss: 0.3241 - val_acc: 0.9012\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 2s 242us/sample - loss: 0.3126 - acc: 0.9046\n",
      "312/312 [==============================] - 57s 181ms/step - loss: 0.3136 - acc: 0.9055 - val_loss: 0.3113 - val_acc: 0.9046\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 3s 350us/sample - loss: 0.3038 - acc: 0.9089\n",
      "312/312 [==============================] - 57s 184ms/step - loss: 0.3141 - acc: 0.9046 - val_loss: 0.3018 - val_acc: 0.9089\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 3s 280us/sample - loss: 0.3044 - acc: 0.9087\n",
      "312/312 [==============================] - 62s 198ms/step - loss: 0.3102 - acc: 0.9066 - val_loss: 0.3032 - val_acc: 0.9087\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 3s 257us/sample - loss: 0.2915 - acc: 0.9128\n",
      "312/312 [==============================] - 57s 182ms/step - loss: 0.3123 - acc: 0.9065 - val_loss: 0.2900 - val_acc: 0.9128\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 2s 215us/sample - loss: 0.2924 - acc: 0.9110\n",
      "312/312 [==============================] - 55s 177ms/step - loss: 0.3056 - acc: 0.9091 - val_loss: 0.2916 - val_acc: 0.9110\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    model_3.fit_generator(my_generator()\n",
    "                    , train_dataset.shape[0]//batch_size//5\n",
    "                    , epochs=5\n",
    "                , validation_data=(valid_dataset,valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 305us/sample - loss: 0.1306 - acc: 0.9621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1306341567747295, 0.9621]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.evaluate(test_dataset,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let us give 3 more rounds to mode_3 which would logically equivalent to the way traying was done for model_1.\n",
    "i.e. training entire training dataset 5 times through 5 epochs  == 5 times full batches; with one full set of batches through 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 2s 244us/sample - loss: 0.2939 - acc: 0.9152\n",
      "312/312 [==============================] - 50s 160ms/step - loss: 0.2533 - acc: 0.9214 - val_loss: 0.2929 - val_acc: 0.9152\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 2s 220us/sample - loss: 0.2844 - acc: 0.9153\n",
      "312/312 [==============================] - 50s 159ms/step - loss: 0.2691 - acc: 0.9173 - val_loss: 0.2842 - val_acc: 0.9153\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 2s 204us/sample - loss: 0.2776 - acc: 0.9166\n",
      "312/312 [==============================] - 50s 161ms/step - loss: 0.2626 - acc: 0.9210 - val_loss: 0.2761 - val_acc: 0.9166\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 0.2797 - acc: 0.9179s - loss: 0.2815 -\n",
      "312/312 [==============================] - 50s 162ms/step - loss: 0.2589 - acc: 0.9208 - val_loss: 0.2790 - val_acc: 0.9179\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 2s 240us/sample - loss: 0.2785 - acc: 0.9174\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 0.2705 - acc: 0.9197 - val_loss: 0.2764 - val_acc: 0.9174\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 2s 225us/sample - loss: 0.2830 - acc: 0.9160\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 0.2203 - acc: 0.9324 - val_loss: 0.2806 - val_acc: 0.9160\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 2s 236us/sample - loss: 0.2773 - acc: 0.9173\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 0.2254 - acc: 0.9295 - val_loss: 0.2754 - val_acc: 0.9173\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 2s 248us/sample - loss: 0.2711 - acc: 0.9195\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 0.2272 - acc: 0.9297 - val_loss: 0.2698 - val_acc: 0.9195\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 2s 211us/sample - loss: 0.2746 - acc: 0.9210\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 0.2215 - acc: 0.9313 - val_loss: 0.2742 - val_acc: 0.9210\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 2s 227us/sample - loss: 0.2713 - acc: 0.9208\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 0.2274 - acc: 0.9304 - val_loss: 0.2691 - val_acc: 0.9208\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    model_3.fit_generator(my_generator()\n",
    "                    , train_dataset.shape[0]//batch_size//5\n",
    "                    , epochs=5\n",
    "                , validation_data=(valid_dataset,valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 367us/sample - loss: 0.1091 - acc: 0.9678s - loss: 0.1075 - ac\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10911551758013666, 0.9678]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.evaluate(test_dataset,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is mere conincidental that batches gave early orientation to the final result. We just saw the snapshot of the entire dataset training. even model1 would have same result at the same snapshot. \n",
    "\n",
    "* We should make sure that each batch has balanced labels. We know for sure we have balanced dataset; we should extend the same in each batches.\n",
    "\n",
    "* If possible we should also try find the outlier and try a technique by name batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
